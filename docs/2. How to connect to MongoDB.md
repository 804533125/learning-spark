# 如何导入数据

数据可能有各种格式，虽然常见的是`HDFS`，但是因为在Python爬虫中数据库用的比较多的是`MongoDB`，所以这里会重点说说如何用spark导入`MongoDB`中的数据。

当然，首先你需要在自己电脑上安装spark环境，简单说下，在这里[下载spark](http://spark.apache.org/downloads.html)，同时需要配置好`JAVA`，`Scala`环境。

这里建议使用`Jupyter notebook`，会比较方便，在环境变量中这样设置

```bash
PYSPARK_DRIVER_PYTHON=jupyter PYSPARK_DRIVER_PYTHON_OPTS=notebook ./bin/pyspark
```

如果你的环境中有多个Python版本，同样可以制定你想要使用的解释器，我这里是`python36`，根据需求修改。

```bash
PYSPARK_PYTHON=/usr/bin/python36
```

# 启动命令

进入spark根目录，`./bin/pyspark`这是最简单的启动命令，默认会打开Python的交互式解释器，但是由于我们上面有设置过，会打开`Jupyter notebook`，接下来变成会方便很多。

先来看看最简单的例子：

```bash
>>> textFile = spark.read.text("README.md")

>>> textFile.count()  # Number of rows in this DataFrame
126

>>> textFile.first()  # First row in this DataFrame
Row(value=u'# Apache Spark')

>>> linesWithSpark = textFile.filter(textFile.value.contains("Spark"))


>>> textFile.filter(textFile.value.contains("Spark")).count()  # How many lines contain "Spark"?
15
```

这里有我之前写过的例子：

















